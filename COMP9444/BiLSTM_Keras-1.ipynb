{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "GBCG3RX9r20w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def clean_text(data):\n",
        "    data=re.sub(r\"(#[\\d\\w\\.]+)\", '', data)\n",
        "    data=re.sub(r\"(@[\\d\\w\\.]+)\", '', data)\n",
        "    data=word_tokenize(data)\n",
        "    return data\n",
        "\n",
        "def clean_numbers(x):\n",
        "    if bool(re.search(r'\\d', x)):\n",
        "        x = re.sub('[0-9]{5,}', '#####', x)\n",
        "        x = re.sub('[0-9]{4}', '####', x)\n",
        "        x = re.sub('[0-9]{3}', '###', x)\n",
        "        x = re.sub('[0-9]{2}', '##', x)\n",
        "    return x\n",
        "    \n",
        "def create_embedding_matrix(filepath,word_index,embedding_dim):\n",
        "    vocab_size=len(word_index)+1\n",
        "    embedding_matrix=np.zeros((vocab_size,embedding_dim))\n",
        "    with open(filepath, encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            word,*vector=line.split()\n",
        "            if word in word_index:\n",
        "                idx=word_index[word]\n",
        "                embedding_matrix[idx] = np.array(vector,dtype=np.float32)[:embedding_dim]\n",
        "    return embedding_matrix\n"
      ],
      "metadata": {
        "id": "_4rVDG0e85sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Building and training model"
      ],
      "metadata": {
        "id": "UkEErS30r1wC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "CBzKlsAH8V9S",
        "outputId": "173fb0bc-dda2-4d55-c8db-56c8ea90f7cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-aa8866b49560>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.txt'"
          ]
        }
      ],
      "source": [
        "# Source: https://medium.com/analytics-vidhya/song-recommendation-based-on-textual-and-facial-emotion-recognition-a95e6259c5d8\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding,Bidirectional,LSTM,GRU,Dense\n",
        "import nltk\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "# from helper import clean_text, create_embedding_matrix\n",
        "\n",
        "nltk.download('punkt')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "f=open('train.txt','r', encoding=\"utf8\")\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_train.append(l[1].strip())\n",
        "    x_train.append(l[0])\n",
        "f=open('test.txt','r', encoding=\"utf8\")\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_test.append(l[1].strip())\n",
        "    x_test.append(l[0])\n",
        "f=open('val.txt','r', encoding=\"utf8\")\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_test.append(l[1].strip())\n",
        "    x_test.append(l[0])\n",
        "data_train=pd.DataFrame({'Text':x_train,'Emotion':y_train})\n",
        "data_test=pd.DataFrame({'Text':x_test,'Emotion':y_test})\n",
        "data=data_train.append(data_test,ignore_index=True)\n",
        "\n",
        "texts=[' '.join(clean_text(text)) for text in data.Text]\n",
        "texts_train=[' '.join(clean_text(text)) for text in x_train]\n",
        "texts_test=[' '.join(clean_text(text)) for text in x_test]\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequence_train=tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test=tokenizer.texts_to_sequences(texts_test)\n",
        "index_of_words=tokenizer.word_index\n",
        "vocab_size=len(index_of_words)+1\n",
        "\n",
        "num_classes=6\n",
        "embed_num_dims=300\n",
        "max_seq_len=500\n",
        "class_names=['anger','sadness','fear','joy','surprise','love']\n",
        "X_train_pad=tf.keras.preprocessing.sequence.pad_sequences(sequence_train,maxlen=max_seq_len)\n",
        "X_test_pad=tf.keras.preprocessing.sequence.pad_sequences(sequence_test,maxlen=max_seq_len)\n",
        "encoding={'anger':0,'sadness':1,'fear':2,'joy':3,'surprise':4,'love':5}\n",
        "y_train=[encoding[x] for x in data_train.Emotion]\n",
        "y_test=[encoding[x] for x in data_test.Emotion]\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)\n",
        "fname='embeddings/wiki-news-300d-1M.vec'\n",
        "embedd_matrix=create_embedding_matrix(fname,index_of_words,embed_num_dims)\n",
        "\n",
        "embedd_layer=Embedding(vocab_size,embed_num_dims,input_length=max_seq_len,weights=[embedd_matrix],trainable=False)\n",
        "gru_output_size=128\n",
        "bidirectional=True\n",
        "model=Sequential()\n",
        "model.add(embedd_layer)\n",
        "model.add(Bidirectional(GRU(units=gru_output_size,dropout=0.2,recurrent_dropout=0.2)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "batch_size=128\n",
        "epochs=8\n",
        "hist=model.fit(X_train_pad,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test_pad,y_test))\n",
        "\n",
        "tf.keras.models.save_model(model,'textmodel',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inputting songs"
      ],
      "metadata": {
        "id": "YKaGXIZfsioY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import load_model\n",
        "import nltk\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "# from LSTM_Keras.helper import clean_text, create_embedding_matrix\n",
        "\n",
        "nltk.download('punkt')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "f=open('LSTM_Keras/train.txt','r', encoding=\"utf8\")\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_train.append(l[1].strip())\n",
        "    x_train.append(l[0])\n",
        "f=open('LSTM_Keras/test.txt','r', encoding=\"utf8\")\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_test.append(l[1].strip())\n",
        "    x_test.append(l[0])\n",
        "f=open('LSTM_Keras/val.txt','r', encoding=\"utf8\")\n",
        "for i in f:\n",
        "    l=i.split(';')\n",
        "    y_test.append(l[1].strip())\n",
        "    x_test.append(l[0])\n",
        "data_train=pd.DataFrame({'Text':x_train,'Emotion':y_train})\n",
        "data_test=pd.DataFrame({'Text':x_test,'Emotion':y_test})\n",
        "data=data_train.append(data_test,ignore_index=True)\n",
        "\n",
        "texts=[' '.join(clean_text(text)) for text in data.Text]\n",
        "texts_train=[' '.join(clean_text(text)) for text in x_train]\n",
        "texts_test=[' '.join(clean_text(text)) for text in x_test]\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequence_train=tokenizer.texts_to_sequences(texts_train)\n",
        "sequence_test=tokenizer.texts_to_sequences(texts_test)\n",
        "index_of_words=tokenizer.word_index\n",
        "vocab_size=len(index_of_words)+1\n",
        "\n",
        "max_seq_len=500\n",
        "class_names=['anger','sadness','fear','joy','surprise','love']\n",
        "model = load_model('LSTM_Keras/textmodel', custom_objects=None, compile=True, options=None)\n",
        "\n",
        "def detect(lyric):\n",
        "    message=[]\n",
        "    message.append(lyric)\n",
        "    seq=tokenizer.texts_to_sequences(message)\n",
        "    padded=tf.keras.preprocessing.sequence.pad_sequences(seq,maxlen=max_seq_len)\n",
        "    pred=model.predict(padded)\n",
        "    print('Emotion:',class_names[np.argmax(pred)])"
      ],
      "metadata": {
        "id": "wM0hFNcs6PhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0anK-uXx84ok",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "f1898d2f-100a-4fbe-ecbe-7326405565ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ea017c41fe00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpython3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'python3' is not defined"
          ]
        }
      ]
    }
  ]
}